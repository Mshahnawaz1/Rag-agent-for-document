{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2551b13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "from utils import document_loader, chunking, vectorstore, format_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcf139f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PERSIST_DIRECTORY = \"../data/chroma_db\" \n",
    "\n",
    "TEMPLATE = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum with maximum of 500 words. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {input}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "class Rag():\n",
    "    def __init__(self, template=TEMPLATE):\n",
    "        self.llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
    "        self.embedding = GoogleGenerativeAIEmbeddings(\n",
    "            model=\"models/text-embedding-004\",\n",
    "            task_type=\"retrieval_document\"  # Optimized for search\n",
    "        )\n",
    "        \n",
    "        self.vectordb = None\n",
    "        self.QAprompt = ChatPromptTemplate.from_template(template=template)\n",
    "        self.qa_chain = None\n",
    "\n",
    "         # Share state from disk to uvicorn workers, if any. \n",
    "        if os.path.exists(PERSIST_DIRECTORY):\n",
    "            try:\n",
    "                temp_db = Chroma(\n",
    "                    persist_directory=PERSIST_DIRECTORY, \n",
    "                    embedding_function=self.embedding\n",
    "                )\n",
    "                \n",
    "                if temp_db._collection.count() > 0:\n",
    "                    self.vectordb = temp_db\n",
    "                    print(f\"Loaded existing vector store with {self.vectordb._collection.count()} chunks.\")\n",
    "                    \n",
    "                    # Qa chain is initialized\n",
    "                    self._initialize_lcel_chain(context=\"\")\n",
    "\n",
    "                else:\n",
    "                    print(\"Persistent directory exists but the Chroma collection is empty.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading existing Chroma DB: {e}. Starting with empty state.\")\n",
    "\n",
    "    def _load_docs(self, file_path: str):\n",
    "        loaded = document_loader(file_path)\n",
    "        chunked = chunking(loaded)\n",
    "        self.vectordb = vectorstore(chunked, self.embedding, PERSIST_DIRECTORY)\n",
    "        \n",
    "        self._initialize_lcel_chain()\n",
    "        print(f\"Document loaded and vector store created at {PERSIST_DIRECTORY}\")\n",
    "\n",
    "    def _initialize_lcel_chain(self, context: str = \"\"):\n",
    "        if not self.vectordb:\n",
    "            print(\"Cannot initialize chain: VectorDB is not loaded.\")\n",
    "            return\n",
    "        \n",
    "        rag_chain_from_docs = (\n",
    "            RunnablePassthrough.assign(\n",
    "                # context=lambda x: format_docs(x['context'])\n",
    "                context= self._retriever_info(question=x[\"question\"])\n",
    "            )\n",
    "            | self.QAprompt  \n",
    "            | self.llm\n",
    "            | StrOutputParser() # Parse the output to a string\n",
    "        )\n",
    "\n",
    "        self.qa_chain = RunnableParallel(\n",
    "            answer=rag_chain_from_docs,\n",
    "            input=RunnablePassthrough()\n",
    "        )\n",
    "    def _retriever_info(self, question: str):\n",
    "        if not self.vectordb:\n",
    "            return \"No VectorDB loaded.\"\n",
    "        results = self.vectordb.similarity_search(\"\", k=3)\n",
    "        return \"\\n\\n\".join([doc.page_content for doc in results])\n",
    "\n",
    "    def _clear_db(self):\n",
    "        if self.vectordb:\n",
    "            try:\n",
    "                self.vectordb.delete_collection()\n",
    "                self.vectordb = None\n",
    "                message = \"Chroma DB cleared successfully.\"\n",
    "            except Exception as e:\n",
    "                message = (f\"Error clearing Chroma DB: {e}\")\n",
    "            return (message)\n",
    "        \n",
    "\n",
    "    def ask(self, question: str):\n",
    "        if self.qa_chain is None:\n",
    "            return {\n",
    "                'status_code': 400,\n",
    "                'response': \"No documents loaded. Please load documents first.\",\n",
    "                'sources': []\n",
    "            }\n",
    "        result = self.qa_chain.invoke({\"question\": question})\n",
    "        print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ac4125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing vector store with 9 chunks.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'page_content'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m engine = Rag()\n\u001b[32m      6\u001b[39m engine._clear_db()\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_load_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m     10\u001b[39m     que = \u001b[38;5;28minput\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mWhat is your question?\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mRag._load_docs\u001b[39m\u001b[34m(self, docs)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_load_docs\u001b[39m(\u001b[38;5;28mself\u001b[39m, docs):\n\u001b[32m     50\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Assumes docs is a list of langchain Documents already split.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28mself\u001b[39m.vectordb = \u001b[43mChroma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m        \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPERSIST_DIRECTORY\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28mself\u001b[39m._initialize_lcel_chain()\n\u001b[32m     57\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ Vector store created at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPERSIST_DIRECTORY\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\codes\\rag\\venv\\Lib\\site-packages\\langchain_chroma\\vectorstores.py:1384\u001b[39m, in \u001b[36mChroma.from_documents\u001b[39m\u001b[34m(cls, documents, embedding, ids, collection_name, persist_directory, host, port, headers, chroma_cloud_api_key, tenant, database, client_settings, client, collection_metadata, collection_configuration, ssl, **kwargs)\u001b[39m\n\u001b[32m   1330\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_documents\u001b[39m(\n\u001b[32m   1332\u001b[39m     \u001b[38;5;28mcls\u001b[39m: \u001b[38;5;28mtype\u001b[39m[Chroma],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1350\u001b[39m     **kwargs: Any,\n\u001b[32m   1351\u001b[39m ) -> Chroma:\n\u001b[32m   1352\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create a Chroma vectorstore from a list of documents.\u001b[39;00m\n\u001b[32m   1353\u001b[39m \n\u001b[32m   1354\u001b[39m \u001b[33;03m    If a persist_directory is specified, the collection will be persisted there.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1382\u001b[39m \u001b[33;03m        Chroma: Chroma vectorstore.\u001b[39;00m\n\u001b[32m   1383\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1384\u001b[39m     texts = [\u001b[43mdoc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpage_content\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m   1385\u001b[39m     metadatas = [doc.metadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m   1386\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'page_content'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e449d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "PERSIST_DIRECTORY = \"../data/chroma_db\"\n",
    "\n",
    "TEMPLATE = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Keep the answer concise (max 3 sentences, 500 words). \n",
    "Always end the answer with \"thanks for asking!\".\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "class Rag:\n",
    "    def __init__(self, template=TEMPLATE):\n",
    "        self.llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
    "        self.embedding = GoogleGenerativeAIEmbeddings(\n",
    "            model=\"models/text-embedding-004\",\n",
    "            task_type=\"retrieval_document\"\n",
    "        )\n",
    "\n",
    "        self.vectordb = None\n",
    "        self.QAprompt = ChatPromptTemplate.from_template(template)\n",
    "        self.qa_chain = None\n",
    "\n",
    "        if os.path.exists(PERSIST_DIRECTORY):\n",
    "            try:\n",
    "                temp_db = Chroma(\n",
    "                    persist_directory=PERSIST_DIRECTORY,\n",
    "                    embedding_function=self.embedding\n",
    "                )\n",
    "                if temp_db._collection.count() > 0:\n",
    "                    self.vectordb = temp_db\n",
    "                    print(f\"Loaded existing vector store with {self.vectordb._collection.count()} chunks.\")\n",
    "                    self._initialize_lcel_chain()\n",
    "                else:\n",
    "                    print(\"Persistent directory exists but collection is empty.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading existing Chroma DB: {e}. Starting fresh.\")\n",
    "\n",
    "    def _load_docs(self, file_path: str):\n",
    "        loaded = document_loader(file_path)\n",
    "        chunked = chunking(loaded)\n",
    "\n",
    "        \"\"\"Assumes docs is a list of langchain Documents already split.\"\"\"\n",
    "        self.vectordb = Chroma.from_documents(\n",
    "            documents=chunked,\n",
    "            embedding=self.embedding,\n",
    "            persist_directory=PERSIST_DIRECTORY\n",
    "        )\n",
    "        self._initialize_lcel_chain()\n",
    "        print(f\"✅ Vector store created at {PERSIST_DIRECTORY}\")\n",
    "\n",
    "    def _retriever_info(self, question: str):\n",
    "        if not self.vectordb:\n",
    "            return \"No VectorDB loaded.\"\n",
    "        results = self.vectordb.similarity_search(question, k=3)\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in results)\n",
    "\n",
    "    def _initialize_lcel_chain(self):\n",
    "        if not self.vectordb:\n",
    "            print(\"Cannot initialize chain: VectorDB not loaded.\")\n",
    "            return\n",
    "\n",
    "        self.qa_chain = (\n",
    "            RunnablePassthrough.assign(\n",
    "                context=lambda x: self._retriever_info(x[\"question\"])\n",
    "            )\n",
    "            | self.QAprompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "    def _clear_db(self):\n",
    "        if self.vectordb:\n",
    "            try:\n",
    "                self.vectordb.delete_collection()\n",
    "                self.vectordb = None\n",
    "                return \"✅ Chroma DB cleared successfully.\"\n",
    "            except Exception as e:\n",
    "                return f\"Error clearing DB: {e}\"\n",
    "\n",
    "    def ask(self, question: str):\n",
    "        if not self.qa_chain:\n",
    "            return {\n",
    "                \"status_code\": 400,\n",
    "                \"response\": \"No documents loaded. Please load documents first.\"\n",
    "            }\n",
    "\n",
    "        answer = self.qa_chain.invoke({\"question\": question})\n",
    "        return {\"status_code\": 200, \"response\": answer}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2350c8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing vector store with 9 chunks.\n",
      "-----Loading Document-----\n",
      "✅ Vector store created at ../data/chroma_db\n",
      "{'status_code': 200, 'response': 'This context describes an internship experience, including an internship certificate, report, and project work. It details the skills gained in AI, automation, data analytics, Python programming, and Machine Learning. The internship focused on applying data-driven techniques to solve real-world problems.\\nthanks for asking!'}\n",
      "{'status_code': 200, 'response': \"I don't know the answer because the question is missing. Please provide the question you would like me to answer based on the context. thanks for asking!\"}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m engine._load_docs(path)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     que = \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is your question?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     result = engine.ask(que)\n\u001b[32m     12\u001b[39m     \u001b[38;5;28mprint\u001b[39m(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\codes\\rag\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py:1275\u001b[39m, in \u001b[36mKernel.raw_input\u001b[39m\u001b[34m(self, prompt)\u001b[39m\n\u001b[32m   1273\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1274\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[32m-> \u001b[39m\u001b[32m1275\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1276\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1277\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1278\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1279\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1280\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\codes\\rag\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py:1320\u001b[39m, in \u001b[36mKernel._input_request\u001b[39m\u001b[34m(self, prompt, ident, parent, password)\u001b[39m\n\u001b[32m   1317\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1318\u001b[39m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[32m   1319\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mInterrupted by user\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1321\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1322\u001b[39m     \u001b[38;5;28mself\u001b[39m.log.warning(\u001b[33m\"\u001b[39m\u001b[33mInvalid Message:\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":    \n",
    "\n",
    "    path = \"../data/uploads/inte.pdf\"\n",
    "    engine = Rag()\n",
    "    engine._clear_db()\n",
    "\n",
    "    engine._load_docs(path)\n",
    "    while(True):\n",
    "        que = input(\"What is your question?\")\n",
    "        result = engine.ask(que)\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efde42d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status_code': 200,\n",
       " 'response': \"I don't know the answer because the question is missing. Please provide the question you would like me to answer based on the context. thanks for asking!\"}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df9af1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: LangChain helps developers build LLM-based\n",
      "Chunk 2: LLM-based applications.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Example input document(s)\n",
    "docs = [Document(page_content=\"LangChain helps developers build LLM-based applications.\")]\n",
    "\n",
    "# Create a text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "# Split the documents into chunks\n",
    "chunked_documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# Verify\n",
    "for i, chunk in enumerate(chunked_documents, 1):\n",
    "    print(f\"Chunk {i}: {chunk.page_content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe77f81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
