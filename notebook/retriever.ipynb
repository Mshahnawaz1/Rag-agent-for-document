{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f212e48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_chroma import Chroma\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "# 3.1 Initialize the Embedding Model\n",
    "# NOTE: The model and task type are crucial for retrieval accuracy.\n",
    "embedding = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/text-embedding-004\",  # A strong general-purpose model\n",
    "    task_type=\"retrieval_document\" # Optimizes the embedding for document search\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42b8fcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -U langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87624217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418\n"
     ]
    }
   ],
   "source": [
    "v1 = Chroma(\n",
    "    persist_directory='../data/chroma/',\n",
    "    embedding_function=embedding\n",
    ")\n",
    "print(v1._collection.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0502e3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac26d3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scientific consensus is that time began with the Big Bang, approximately 13.8 billion years ago. Before this event, the concepts of space and time as we understand them did not exist. There is no \"before\" the Big Bang in a temporal sense, as time itself emerged with the universe.\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"User: {input}\n",
    "Make sure your renponse is concise and clear and do not exceed three sentences.\n",
    "\"\"\"\n",
    "\n",
    "class llmPipeline():\n",
    "    def __init__(self, model=\"gemini-2.5-flash\", prompt_template=template):\n",
    "        self.llm = ChatGoogleGenerativeAI(model=model)\n",
    "        self.prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "        self.chain = self.prompt | self.llm | StrOutputParser()\n",
    "\n",
    "    def invoke(self, input_dict={}):\n",
    "        try:\n",
    "            response = self.chain.invoke(input_dict)\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            return f\"API Test Failed. Ensure the GOOGLE_API_KEY is correctly set in your .env file. Error: {e}\"\n",
    "a = llmPipeline()\n",
    "\n",
    "result = a.invoke({\"input\": \"what is the beginign of time?\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11247d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d1de578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The context describes a technician feeling unjustly accused for efficiently doing his job, while another scene features a person interacting with a policeman to ensure a subsidy is not cut off, sending greetings to the Governor. The policeman, nearing the end of his shift, is focused on paperwork and suggests a follow-up meeting. Separately, a reflective passage discusses gaining knowledge from books and food, recognizing the temporary nature of one's stay. thanks for asking!\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build prompt\n",
    "from langchain.prompts import PromptTemplate\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"],template=template,)\n",
    "\n",
    "# Run chain\n",
    "from langchain.chains import RetrievalQA\n",
    "question = \"Summarize the context of the documents in three sentences.\"\n",
    "qa_chain = RetrievalQA.from_chain_type(llm,\n",
    "                                       retriever=v1.as_retriever(),\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT})\n",
    "\n",
    "\n",
    "result = qa_chain({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c874a2f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function str.endswith>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = \"..data/text_files/kf100\"\n",
    "\n",
    "s = s1.endswith()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7639ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
